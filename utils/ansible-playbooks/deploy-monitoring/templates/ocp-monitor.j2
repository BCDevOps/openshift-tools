#!/usr/bin/env bash

# Exit on error. Append "|| true" if you expect an error.
set -o errexit
# Exit on error inside any functions or subshells.
set -o errtrace
# Do not allow use of undefined vars. Use ${VAR:-} to use an undefined VAR
set -o nounset
# Catch the error in case mysqldump fails (but gzip succeeds) in `mysqldump |gzip`
set -o pipefail
# Turn on traces, useful while debugging but commented out by default
# set -o xtrace

# Print a backtrace on error
# https://github.com/ab/bin/blob/master/bash-backtrace.sh
{% raw %}
bash_backtrace() {
    # Return value of command that caused error
    local RET=$?
    # Frame counter
    local I=0
    # backtrace layers
    local FRAMES=${#BASH_SOURCE[@]}

    # Restore STDOUT and STDERR as they might be in unknown states due to catching
    # an error in the middle of a command
    exec 1>&3 2>&4

    echo "Traceback (most recent call last):"

    for ((FRAME=FRAMES-2; FRAME >= 0; FRAME--)); do
        local LINENO=${BASH_LINENO[FRAME]}

        # Location of error
        echo "  File ${BASH_SOURCE[FRAME+1]}, line ${LINENO}, in ${FUNCNAME[FRAME+1]}"

        # Print the error line, with preceding whitespace removed
        echo "$(sed -n "${LINENO}s/^[   ]*/    /p" "${BASH_SOURCE[FRAME+1]}")"
    done

    echo "Exiting with status ${RET}"
    exit "${RET}"
}

## FUNCTIONS FOR STATE MACHINE CHECKS

function get_current_state () {
# Parameters passed:
#  - unique name identifying what is being tracked state-wise
#  - variable to hold the current state, either read from state file or initialized when not yet set.
# Also forms the basis for the file-name used to track state.
# If the file does not exist, then a state of 0 is assigned and the state file created.

  local statename="$1"
  local statevalue="$2"
  local current_state=0

  STATE_FILE="/tmp/ocp-monitor."$1".state"

  if [ -f $STATE_FILE ]; then
    current_state=`cat $STATE_FILE`;
  else
    current_state=`echo 0 | tee $STATE_FILE`
  fi
  eval $statevalue=$current_state
}

function set_state () {
# Parameters passed:
#  - unique name identifying what is being tracked state-wise
#  - variable to hold the current state, either read from state file or initialized when not yet set.
# Also forms the basis for the file-name used to track state.
# If the file does not exist, then a state of 0 is assigned and the state file created.

  STATE_FILE="/tmp/ocp-monitor."$1".state"
  local evalstate = `echo $2 | tee $STATE_FILE`
}

## FUNCTIONS FOR NODE_READY CHECKS
# These checks used to be placed in-line, but are now separated into separate functions so
# as to increase clarity when adding the state checks for Node Ready health check.

function test_if_all_nodes_are_ready () {
# following test will return 1 if all nodes have a Ready state = True. Otherwise, return 0.

  if oc get nodes -o go-template="{{range .items}}{{.metadata.name}}{{print \"\t\"}}{{range .status.conditions}}{{if eq .type \"Ready\"}}{{.status}}{{end}}{{end}}{{print \"\n\"}}{{end}}" | grep -v True
  then
    return 0
  else
    return 1
  fi
}

function return_node_ready_count () {
# counts and returns the number of nodes where Ready=True.
# works around limits involving functions by passing by reference update to the calling parameter to the function.

  local var1="$1"
  eval $var1=$(oc get nodes -o go-template="{{range .items}}{{.metadata.name}}{{print \"\t\"}}{{range .status.conditions}}{{if eq .type \"Ready\"}}{{.status}}{{end}}{{end}}{{print \"\n\"}}{{end}}"|wc -l)
}

{% endraw %}
# Copy STDOUT and STDERR so they can be restored later
exec 3>&1 4>&2
# Trap script errors and print some helpful debug info
trap bash_backtrace ERR

# Standardize the date stamps output
function datestamp () {
  date "+%F %H:%M:%S"
}

# Test that the API is responding
echo -n "$(datestamp) OpenShift API Status: "
if ! curl --silent --show-error {{ api_url }}/healthz | grep ok
then
  echo "$(datestamp) ERROR: curl of API {{ api_url }}/healthz failed"
fi

{% if ansible_fqdn == primary_master %}
# Test that the Router service is responding
# TODO switch to DNS name once server are using dnsmasq properly
echo -n "$(datestamp) OpenShift Router Status: "
if ! curl --silent --show-error {{ router_service }}/healthz | grep 'Service ready'
then
  echo "$(datestamp) ERROR: curl of router service {{ router_service }}/healthz failed"
fi

# Test that a sample route is working
echo -n "$(datestamp) Canary Route Status: "
if ! curl --head --insecure --silent --show-error {{ canary_route }} | grep '200 OK'
then
  echo "$(datestamp) ERROR: curl of {{ canary_route }} failed"
fi

# Test the Docker Registry is responding
# TOD switch to DNS name once server is using dnsmasq properly
echo -n "$(datestamp) Docker Registry Status: "
if ! curl --head --silent --show-error {{ registry_service }}/healthz | grep '200 OK'
then
  echo "$(datestamp) ERROR: curl of docker registry service {{ registry_service }}/healthz failed"
fi

# Run provided metrics health check
# Find the dir we are running in
DIR=$(dirname $(readlink -f "$0"))
echo -n "$(datestamp) Metrics status: "
if ! /usr/local/bin/metrics-health-check {{ metrics_url }}
then
  echo "$(datestamp) ERROR: Metrics health check failed"
fi

# Check disk space of Registry
echo -n "$(datestamp) Registry disk usage: "
POD=$(oc -n default get pods -l deploymentconfig=docker-registry -o name | sed 's#pods/##' | head -n 1)
SPACE=$(oc -n default exec $POD -- sh -c 'echo "$(df /registry | awk '\''/registry/ {print $5}'\'' | tr -d "%")"')
echo "$(echo 100 '-' $SPACE | bc -l)% free"
FREE=$(echo 100 '-' $SPACE '>' 15 | bc -l)
if [ "$FREE" -lt 1 ]
then
  echo "$(datestamp) ERROR: Low Docker Registry Space"
fi

# Check disk space of Metrics Cassandra
echo -n "$(datestamp) Metrics disk usage: "
POD=$(oc -n openshift-infra get pods -l type=hawkular-cassandra -o name | sed 's#pods/##' | head -n 1)
SPACE=$(oc -n openshift-infra exec $POD -- sh -c 'echo "$(df /cassandra_data | awk '\''/cassandra_data/ {print $5}'\'' | tr -d "%")"')
echo "$(echo 100 '-' $SPACE | bc -l)% free"
FREE=$(echo 100 '-' $SPACE '>' 15 | bc -l)
if [ "$FREE" -lt 1 ]
then
  echo "$(datestamp) ERROR: Low Metrics Cassandra Space"
fi

{% raw %}
# Check that all nodes are Ready
# Now with additional Finite State Automata support
# One failure in a row will only log WARN to log file. Two or
# more consecutive failures in a row will trigger ERROR alert.
echo "$(datestamp) Check for NodeNotReady"
get_current_state nodestatus 'NODE_READY_STATE'

if [[ "$(test_if_all_nodes_are_ready)" = "0" ]]; then
  if [[ $NODE_READY_STATE -eq "0" ]]; then
# node ready failure while in state 0, bumps us to state 1 and warns only
    set_state nodestatus 1
    echo "$(datestamp) WARN: Some nodes are not Ready"
  elif [[ $NODE_READY_STATE -eq "1" ]]; then
# node ready failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state nodestatus 2
    echo "$(datestamp) ERROR: Some nodes are not Ready"
  elif [[ $NODE_READY_STATE -eq "2" ]]; then
# node ready failure while in state 2, no state change. Still will alert though.
    echo "$(datestamp) ERROR: Some nodes are not Ready"
  fi
# Dump the events, as they seem to go away quickly
  oc get events -n default
else
# All nodes are ready, grab the node count and then adjust state accordingly.
  return_node_ready_count 'NUM_NODES_READY'
  if [[ $NODE_READY_STATE -eq "0" ]]; then
# node ready success while in state 0, no state change
    echo "$(datestamp) INFO: $NUM_NODES_READY nodes online and ready"
  elif [[ $NODE_READY_STATE -eq "1" ]]; then
# node ready success while in state 1, state change back to 0
    set_state nodestatus 0
    echo "$(datestamp) INFO: $NUM_NODES_READY nodes online and ready"
  elif [[ $NODE_READY_STATE -eq "2" ]]; then
# node ready success while in state 2, state change back to 0
    set_state nodestatus 0
    echo "$(datestamp) INFO: $NUM_NODES_READY nodes online and ready"
  fi
fi

# Verify the Registry is on 3 nodes
NODES=$(oc -n default get pods -l deploymentconfig=docker-registry -o yaml -o go-template="{{range .items}}{{if eq .status.phase \"Running\"}}{{.spec.nodeName}}{{print \"\n\"}}{{end}}{{end}}")
POD_COUNT=$(echo "$NODES" | wc -l)
UNIQ_NODES=$(echo "$NODES" | sort | uniq | wc -l)
echo "$(datestamp) Check that Registry is scaled to 3: $POD_COUNT"
echo "$(datestamp) Check that Registry is scaled to all infra nodes: " $UNIQ_NODES
if [ "$POD_COUNT" -lt 3 ]
then
  echo "$(datestamp) ERROR: Registry pod count $POD_COUNT is less than 3"
elif [ "$UNIQ_NODES" -lt 3 ]
then
  echo "$(datestamp) ERROR: Registry on too few nodes " $NODES
fi

# Verify the Router is on 3 nodes
NODES=$(oc -n default get pods -l deploymentconfig=router -o yaml -o go-template="{{range .items}}{{if eq .status.phase \"Running\"}}{{.spec.nodeName}}{{print \"\n\"}}{{end}}{{end}}")
POD_COUNT=$(echo "$NODES" | wc -l)
UNIQ_NODES=$(echo "$NODES" | sort | uniq | wc -l)
echo "$(datestamp) Check that Router is scaled to 3: $POD_COUNT"
echo "$(datestamp) Check that Router is scaled to all infra nodes: " $UNIQ_NODES
if [ "$POD_COUNT" -lt 3 ]
then
  echo "$(datestamp) ERROR: Router pod count $POD_COUNT is less than 3"
elif [ "$UNIQ_NODES" -lt 3 ]
then
  echo "$(datestamp) ERROR: Router on too few nodes " $NODES
fi

# Verify the IP Failover is on 3 nodes
{% endraw %}
ROUTER_DC="{{ router_dc }}"
{% raw %}
NODES=$(oc -n default get pods -l deploymentconfig="${ROUTER_DC}" -o yaml -o go-template="{{range .items}}{{if eq .status.phase \"Running\"}}{{.spec.nodeName}}{{print \"\n\"}}{{end}}{{end}}")
POD_COUNT=$(echo "$NODES" | wc -l)
UNIQ_NODES=$(echo "$NODES" | sort | uniq | wc -l)
echo "$(datestamp) Check that IPF is scaled to 3: $POD_COUNT"
echo "$(datestamp) Check that IPF is scaled to all infra nodes: " $UNIQ_NODES
if [ "$POD_COUNT" -lt 3 ]
then
  echo "$(datestamp) ERROR: IPF pod count $POD_COUNT is less than 3"
elif [ "$UNIQ_NODES" -lt 3 ]
then
  echo "$(datestamp) ERROR: IPF on too few nodes " $NODES
fi

# Check the clusters capacity
echo "$(datestamp) Check Cluster Capacity of region=app"
# Get all the nodes in region=app
# Format into a string with each node enclosed in quotes
NODE_LIST=$(oc get nodes -l region=app -o name | sed 's#nodes/##')
NODES=
for N in $NODE_LIST; do NODES="$NODES \"$N\""; done
# Get Node capacity
# node.dmz cpu_int mem_ki pods
NODE_CAP=$(oc get nodes -o go-template="{{range .items}}{{if  eq .metadata.name $NODES}}{{.metadata.name}}{{print \"\t\"}}{{.status.capacity.cpu}}{{print \"\t\"}}{{.status.capacity.memory}}{{print \"\t\"}}{{.status.capacity.pods}}{{print \"\n\"}}{{end}}{{end}}")
# CPU Capacity
CPU_CAP=$(echo "$NODE_CAP" | awk -F "\t" 'BEGIN { SUM=0 } {if ($2 ~ "m") SUM+=$2/1000; else SUM+=$2 } END {print SUM}')
# Memory Capacity in Gi
MEM_CAP=$(echo "$NODE_CAP" | awk -F "\t" 'BEGIN { SUM=0 } {if ($3 ~ "Gi") $3=$3*1024"Mi"; if ($3 ~ "G") $3=$3*1000"M"; if ($3 ~ "Mi") $3=$3*1024"Ki"; if ($3 ~ "M") $3=$3*1000"K"; if ($3 ~ "Ki") $3=$3*1024; if ($3 ~ "K") $3=$3*1000; SUM+=$3} END { print SUM/1024/1024/1024"Gi"}')
# Pod Capacity
POD_CAP=$(echo "$NODE_CAP" | awk -F "\t" 'BEGIN { SUM=0 } {SUM+=$4 } END {print SUM}')
# Get stats for running pods
PODS=$(oc get pods --all-namespaces -o go-template="{{range .items}}{{if .spec.nodeName}}{{if and (not (eq .status.phase \"Failed\" \"Succeeded\")) (eq .spec.nodeName $NODES)}}{{range .spec.containers}}{{.name}}{{print \"\t\"}}{{.resources.requests.cpu}}{{print \"\t\"}}{{.resources.requests.memory}}{{print \"\n\"}}{{end}}{{end}}{{end}}{{end}}")
# CPU Requests
CPU_REQ=$(echo "$PODS" | awk -F "\t" 'BEGIN { SUM=0 } {if ($2 ~ "m") SUM+=$2/1000; else SUM+=$2 } END {print SUM}')
# Memory Requests in Gi
MEM_REQ=$(echo "$PODS" | awk -F "\t" 'BEGIN { SUM=0 } {if ($3 ~ "Gi") $3=$3*1024"Mi"; if ($3 ~ "G") $3=$3*1000"M"; if ($3 ~ "Mi") $3=$3*1024"Ki"; if ($3 ~ "M") $3=$3*1000"K"; if ($3 ~ "Ki") $3=$3*1024; if ($3 ~ "K") $3=$3*1000; SUM+=$3} END { print SUM/1024/1024/1024"Gi"}')
# Pods Running
POD_REQ=$(echo "$PODS" | wc -l)
# Print CPU
CPU_PERCENT=$(echo "$CPU_REQ / $CPU_CAP * 100" | bc -l | awk '{printf "%.2f",$0}')
echo "$(datestamp) CPU Capacity: $CPU_REQ / $CPU_CAP = ${CPU_PERCENT}%"
CPU_PERCENT=$(echo "$CPU_PERCENT" | awk '{printf "%.f",$0}')
if [ "$CPU_PERCENT" -gt 85 ]
then
  echo "$(datestamp) ERROR: Cluster low on CPU capacity"
fi
# Print Memory
MEM_PERCENT=$(echo "$MEM_REQ / $MEM_CAP * 100" | sed 's#Gi##g' | bc -l | awk '{printf "%.2f",$0}')
echo "$(datestamp) Memory Capacity: $MEM_REQ / $MEM_CAP = ${MEM_PERCENT}%"
MEM_PERCENT=$(echo "$MEM_PERCENT" | awk '{printf "%.f",$0}')
if [ "$MEM_PERCENT" -gt 85 ]
then
  echo "$(datestamp) ERROR: Cluster low on Memory capacity"
fi
# Print Pods
POD_PERCENT=$(echo "$POD_REQ / $POD_CAP * 100" | bc -l | awk '{printf "%.2f",$0}')
echo "$(datestamp) Pod Capacity: $POD_REQ / $POD_CAP = ${POD_PERCENT}%"
POD_PERCENT=$(echo "$POD_PERCENT" | awk '{printf "%.f",$0}')
if [ "$POD_PERCENT" -gt 85 ]
then
  echo "$(datestamp) ERROR: Cluster low on Pod capacity"
fi
{% endraw %}
{% endif %}

{% if ansible_fqdn in groups['master'] %}
{% raw %}
# Verify ETCD is running
echo -n "$(datestamp) ETCD Deamon Status: "
if ! systemctl is-active etcd.service
then
  echo "$(datestamp) ERROR: ETCD is not running"
fi

# Verify API service is running
echo -n "$(datestamp) OpenShift API Deamon Status: "
if ! systemctl is-active atomic-openshift-master-api.service
then
  echo "$(datestamp) ERROR: OpenShift API is not running"
fi

# Verify Controller service is running
echo -n "$(datestamp) OpenShift Controller Deamon Status: "
if ! systemctl is-active atomic-openshift-master-controllers.service
then
  echo "$(datestamp) ERROR: OpenShift Controller is not running"
fi
{% endraw %}
{% endif %}

{% if ansible_fqdn in groups['storage'] %}
{% raw %}
# Check Gluster disk usage
export LVM_SUPPRESS_FD_WARNINGS=1
echo "$(datestamp) Checking Gluster Disks"
# Find the volume groups
VGS=$(/sbin/vgs --readonly --noheadings -o vg_name | egrep 'vg_[0-9a-f]{32}')
for VG in $VGS
do
  VG_STATS=$(/sbin/vgs --units g --nosuffix --separator " " --readonly --options vg_size,vg_free --noheadings $VG )
  SIZE=$(echo $VG_STATS | awk '{print $1}')
  FREE=$(echo $VG_STATS | awk '{print $2}')
  USED=$(echo "$SIZE - $FREE" | bc -l)
  PERCENT=$(echo "$USED / $SIZE * 100" | bc -l | awk '{printf "%.2f",$0}')
  echo "$(datestamp) $VG = ${USED}G / ${SIZE}G = $PERCENT%"
  PERCENT=$(echo "$PERCENT" | awk '{printf "%.f",$0}')
  if [ "$PERCENT" -gt 85 ]
  then
    echo "$(datestamp) ERROR: Gluster device $VG low on space"
  fi
done
{% endraw %}
{% endif %}

# Checks for all nodes

# Check Docekr is running
echo -n "$(datestamp) Docker Deamon Status: "
if ! systemctl is-active docker.service
then
  echo "$(datestamp) ERROR: Docker is not running"
  # Other checks depend on Docker running, exit out
  exit 1
fi

# Check Docker Data usage
echo -n "$(datestamp) Docker Data Space is: "
SPACE=$(echo $(echo \"$(docker info 2>/dev/null | awk '/Data Space Available/ {print $4}') / $(docker info 2>/dev/null | awk '/Data Space Total/ {print $4}')\" | bc -l))
echo "$SPACE = $(echo $SPACE '*' 100 | bc -l | awk '{printf "%.2f",$0}')% free"
FREE=$(echo $SPACE '>' 0.15 | bc -l)
if [ "$FREE" -lt 1 ]
then
  echo "$(datestamp) ERROR: Low Docker Data Space"
fi

# Check Docker Metadata Usage
echo -n "$(datestamp) Docker Metadata Space is: "
SPACE=$(echo $(echo \"$(docker info 2>/dev/null | awk '/Metadata Space Available/ {print $4}') / $(docker info 2>/dev/null | awk '/Metadata Space Total/ {print $4}')\" | bc -l))
echo "$SPACE = $(echo $SPACE '*' 100 | bc -l | awk '{printf "%.2f",$0}')% free"
FREE=$(echo $SPACE '>' 0.15 | bc -l)
if [ "$FREE" -lt 1 ]
then
  echo "$(datestamp) ERROR: Low Docker Metadata Space"
fi

# Check for local volume count
# May fail on masters that have zero, so ||true
VOLS=$(mount|grep -c openshift.local.volumes||true)
echo "$(datestamp) Pod volumes: $VOLS"
if [ "$VOLS" -gt "1000" ]
then
  echo "$(datestamp) ERROR: Too many volumes"
fi

# Check Node service
echo -n "$(datestamp) OpenShift Node Deamon Status: "
if ! systemctl is-active atomic-openshift-node.service
then
  echo "$(datestamp) ERROR: OpenShift Node is not running"
fi

# Check OpenVSwitch service
echo -n "$(datestamp) OpenVSwitch Deamon Status: "
if ! systemctl is-active openvswitch.service
then
  echo "$(datestamp) ERROR: OpenVSwitch is not running"
fi

