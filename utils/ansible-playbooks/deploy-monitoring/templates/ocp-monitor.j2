#!/usr/bin/env bash

# Exit on error. Append "|| true" if you expect an error.
set -o errexit
# Exit on error inside any functions or subshells.
set -o errtrace
# Do not allow use of undefined vars. Use ${VAR:-} to use an undefined VAR
set -o nounset
# Catch the error in case mysqldump fails (but gzip succeeds) in `mysqldump |gzip`
set -o pipefail
# Turn on traces, useful while debugging but commented out by default
# set -o xtrace

# Global variables re-used through the script.
MSG=""
MAILTO="William.Hutchison@dxcas.com Steven.Barre@dxcas.com Dan.Deane@dxcas.com bcdevex@arctiq.ca Shea.Phillips@gov.bc.ca"
MAILFROM="-r hslinuxteam@dxcas.com"

# Define defaults for Curl
CURL="curl --connect-timeout 5 --max-time 10 --silent --show-error"

# Define global variable representing the text to display in the email subject for environment.
EMAIL_ENV="{{ email_env }}"

# Global variables responsible for tracking state on each Finite State Automata (FSA).
# The default value here does not affect anything - actual use of the FSA will set this
# to the actual state value when queried.
CANARY_HEALTH_STATE="0"
METRICS_HEALTH_STATE="0"
REGISTRY_POD_STATE="0"
ROUTER_POD_STATE="0"
IPF_POD_STATE="0"
CPU_CAPACITY_STATE="0"
MEM_CAPACITY_STATE="0"
POD_CAPACITY_STATE="0"
DOCKER_PROCESS_STATE="0"

# capacity percentage thresholds to check for and warn/alert on.
CPU_CAPACITY_THRESHOLD=85
MEM_CAPACITY_THRESHOLD=85
POD_CAPACITY_THRESHOLD=85
GLUSTER_POD_THRESHOLD=50

{% raw %}
# Print a backtrace on error
# https://github.com/ab/bin/blob/master/bash-backtrace.sh
bash_backtrace() {
    # Return value of command that caused error
    local RET=$?
    # Frame counter
    local I=0
    # backtrace layers
    local FRAMES=${#BASH_SOURCE[@]}

    # Restore STDOUT and STDERR as they might be in unknown states due to catching
    # an error in the middle of a command
    exec 1>&3 2>&4

    echo "Traceback (most recent call last):"

    for ((FRAME=FRAMES-2; FRAME >= 0; FRAME--)); do
        local LINENO=${BASH_LINENO[FRAME]}

        # Location of error
        echo "  File ${BASH_SOURCE[FRAME+1]}, line ${LINENO}, in ${FUNCNAME[FRAME+1]}"
        # Print the error line, with preceding whitespace removed
        echo "$(sed -n "${LINENO}s/^[   ]*/    /p" "${BASH_SOURCE[FRAME+1]}")"
    done

    echo "Exiting with status ${RET}"
    exit "${RET}"
}
{% endraw %}
# Copy STDOUT and STDERR so they can be restored later
exec 3>&1 4>&2
# Trap script errors and print some helpful debug info
trap bash_backtrace ERR

# Standardize the date stamps output
function datestamp () {
  date "+%F %H:%M:%S"
}

## FUNCTIONS FOR STATE MACHINE CHECKS

function get_state () {
# Parameters passed:
#  - STATENAME = uniquei name identifying what is being tracked state-wise. Alllows re-use of this function to
#    implement more than one state automata in the same script if desired. Also acts as the basis for how the state file
#    is named.
#  - STATEVARIABLE = text which is the actual name of the global variable to be passed the value of the current state.
#    If the state of the automata is not yet set, then an initial state of 0 is assigned as a starting point.
#
# IMPORTANT - the use of eval is what allows the passing of a value back to any arbitrary global variable without re-writing
# this function, or have multiple copies. This allows us to use the one function to setup multiple state automata and use the
# current function as-is. Examples of using the same function for multiple state automata:

# get_state first_machine FIRST_MACHINE_VARIABLE
# get_state second_machine SECOND_MACHINE_VARIABLE
#
# First call will grab the state of first_machine and store in the global variable FIRST_MACHINE_VARIABLE.
# Second call will do the same for second_machine, storing in a different global variable named SECOND_MACHINE_VARIABLE.
# For both function calls, the use of eval allows us to not have to re-write that one function despite capturing return values
# to different variables.

  local STATENAME="$1"
  local STATEVARIABLE="$2"
  local CURRENT_STATE=0

  STATE_FILE="/tmp/ocp-monitor."${STATENAME}".state"

  if [ -f "${STATE_FILE}" ]; then
    CURRENT_STATE=`cat "${STATE_FILE}"`;
  else
    CURRENT_STATE=`echo 0 | tee "${STATE_FILE}"`
  fi
  eval "${STATEVARIABLE}"="${CURRENT_STATE}"
}

function set_state () {
# Parameters passed:
#  - STATENAME = unique name identifying what is being tracked state-wise. Alllows re-use of this function to
#    implement more than one state automata in the same script if desired. Also acts as the basis for how the state file
#    is named.
#  - STATEVALUE = actual desired value to set the new state to, usually as the result of a specific check.
#

  local STATENAME="$1"
  local STATEVALUE="$2"
  STATE_FILE="/tmp/ocp-monitor.""${STATENAME}"".state"
  local EVALSTATE=`echo "${STATEVALUE}" | tee "${STATE_FILE}"`
}

## FUNCTIONS FOR NODE_READY CHECKS
# These checks used to be placed in-line, but are now separated into separate functions so
# as to increase clarity when adding the state checks for Node Ready health check.

function test_if_all_nodes_are_ready () {
# following test will return 1 if all nodes have a Ready state = True. Otherwise, return 0.
# Works on data stored in the global variable CURRENT_NODE_STATE to reduce duplication of queries to OpenShift.
  local NODES_READY_CHECK=$(echo "${CURRENT_NODE_STATE}" | grep -v True)
  if [ -z "${NODES_READY_CHECK}" ]; then
    echo 1
  else
    echo 0
  fi
}

function return_node_ready_count () {
# counts and returns the number of nodes where Ready=True.
# Works on data stored in the global variable CURRENT_NODE_STATE to reduce duplication of queries to OpenShift.

  echo $(echo "${CURRENT_NODE_STATE}" | wc -l)
}

## BEGIN TESTS

# Test that the API is responding
echo -n "$(datestamp) OpenShift API Status: "
STATUS=$(${CURL} {{ api_url }}/healthz 2>&1)
if ! echo "${STATUS}" | grep ok
then
  echo -e "${STATUS}"
  MSG=$(echo "$(datestamp) ERROR: curl of API {{ api_url }}/healthz failed")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for API Status Check on `hostname`" $MAILFROM $MAILTO
else
  echo -e "${STATUS}"
fi

##### CHECKS FOR ONLY PRIMARY MASTER NODE #####

{% if ansible_fqdn == primary_master %}
# Test that the Router service is responding
echo -n "$(datestamp) OpenShift Router Status: "
STATUS=$(${CURL} http://router.default.svc:1936/healthz 2>&1)
if ! echo "${STATUS}" | grep -i OK
then
  MSG=$(echo -e "\n$(datestamp) ERROR: curl of router service http://router.default.svc:1936/healthz failed\n${STATUS}")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Router Status Check on `hostname`" $MAILFROM $MAILTO
else
  echo -e "${STATUS}"
fi

# Test that a sample route is working
echo "$(datestamp) Canary Route Status"
get_state canaryhealth CANARY_HEALTH_STATE
# Use --insecure in case there are TLS cert issues
STATUS=$(${CURL} --head --insecure {{ canary_route }} 2>&1)
if ! echo "${STATUS}" | grep '200 OK'
then

  if [[ "${CANARY_HEALTH_STATE}" -eq "0" ]]; then
# canary health check failure while in state 0, bumps us to state 1 and warns only
    set_state canaryhealth 1
    MSG=$(echo -e "$(datestamp) WARN: curl of {{ canary_route }} failed\n${STATUS}")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for CanaryHealthCheck on `hostname`" $MAILFROM $MAILTO
  elif [[ "${CANARY_HEALTH_STATE}" -eq "1" ]]; then
# canary health check failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state canaryhealth 2
    MSG=$(echo -e "$(datestamp) ERROR: curl of {{ canary_route }} failed\n${STATUS}")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for CanaryHealthCheck on `hostname`" $MAILFROM $MAILTO
  elif [[ "${CANARY_HEALTH_STATE}" -eq "2" ]]; then
# canary health check failure while in state 2, no state change. Still will alert though.
    MSG=$(echo -e "$(datestamp) ERROR: curl of {{ canary_route }} failed\n${STATUS}")
    echo -e "${MSG}"
  fi
else
# Canary health check passed, adjust state accordingly.
  if [[ "${CANARY_HEALTH_STATE}" -eq "0" ]]; then
# canary health check success while in state 0, no state change
    echo "$(datestamp) INFO: curl of {{ canary_route }} passed"
  elif [[ "${CANARY_HEALTH_STATE}" -eq "1" ]]; then
# canary health check success while in state 1, state change back to 0
    set_state canaryhealth 0
    echo "$(datestamp) INFO: curl of {{ canary_route }} passed"
  elif [[ "${CANARY_HEALTH_STATE}" -eq "2" ]]; then
# canary health check success while in state 2, state change back to 0
    set_state canaryhealth 0
    echo "$(datestamp) INFO: curl of {{ canary_route }} passed"
  fi
fi
# Test the Docker Registry is responding
# TOD switch to DNS name once server is using dnsmasq properly
echo -n "$(datestamp) Docker Registry Status: "
# Use --head as this enpoint returns an empty document
STATUS=$(${CURL} --head {{ registry_service }}/healthz 2>&1)
if ! echo "${STATUS}" | grep '200 OK'
then
  MSG=$(echo -e"\n$(datestamp) ERROR: curl of docker registry service {{ registry_service }}/healthz failed\n${STATUS}")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Registry Status Check on `hostname`" $MAILFROM $MAILTO
else
  echo -e "${STATUS}"
fi

# Run provided metrics health check
# Find the dir we are running in
DIR=$(dirname $(readlink -f "$0"))
get_state metricshealth METRICS_HEALTH_STATE

echo "$(datestamp) Metrics Status"
if ! STATUS=$(/usr/local/bin/metrics-health-check {{ metrics_url }} 2>&1 | grep '200 Ok'); then

  if [[ "${METRICS_HEALTH_STATE}" -eq "0" ]]; then
# metrics health check failure while in state 0, bumps us to state 1 and warns only
    set_state metricshealth 1
    MSG=$(echo -e "$(datestamp) WARN: Metrics health check failed\n${STATUS}")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for MetricsHealthCheck on `hostname`" $MAILFROM ${MAILTO}
  elif [[ "${METRICS_HEALTH_STATE}" -eq "1" ]]; then
# metrics health check failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state metricshealth 2
    MSG=$(echo -e "$(datestamp) ERROR: Metrics health check failed\n${STATUS}")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for MetricsHealthCheck on `hostname`" $MAILFROM ${MAILTO}
  elif [[ "${METRICS_HEALTH_STATE}" -eq "2" ]]; then
# metrics health check failure while in state 2, no state change. Still will alert though.
    MSG=$(echo "$(datestamp) ERROR: Metrics health check failed")
    echo -e "${MSG}"
  fi
else
# Metrics health check passed, adjust state accordingly.
  if [[ "${METRICS_HEALTH_STATE}" -eq "0" ]]; then
# metrics health check success while in state 0, no state change
    echo "$(datestamp) INFO: Metrics health check passed"
  elif [[ "${METRICS_HEALTH_STATE}" -eq "1" ]]; then
# metrics health check success while in state 1, state change back to 0
    set_state metricshealth 0
    echo "$(datestamp) INFO: Metrics health check passed"
  elif [[ "${METRICS_HEALTH_STATE}" -eq "2" ]]; then
# metrics health check success while in state 2, state change back to 0
    set_state metricshealth 0
    echo "$(datestamp) INFO: Metrics health check passed"
  fi
fi
# Check disk space of Registry
echo -n "$(datestamp) Registry disk usage: "
POD=$(oc -n default get pods -l deploymentconfig=docker-registry --no-headers -o=custom-columns=NAME:.metadata.name | head -n 1)
SPACE=$(oc -n default exec $POD -- sh -c 'echo "$(df /registry | awk '\''/registry/ {print $5}'\'' | tr -d "%")"')
echo "$(echo 100 '-' $SPACE | bc -l)% free"
FREE=$(echo 100 '-' $SPACE '>' 15 | bc -l)
if [ "$FREE" -lt 1 ]
then
  MSG=$(echo "$(datestamp) ERROR: Low Docker Registry Space")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Registry Space on `hostname`" $MAILFROM $MAILTO
fi

# Check disk space of Metrics Cassandra
echo -n "$(datestamp) Metrics disk usage: "
POD=$(oc -n openshift-infra get pods -l type=hawkular-cassandra --no-headers -o=custom-columns=NAME:.metadata.name | head -n 1)
SPACE=$(oc -n openshift-infra exec $POD -- sh -c 'echo "$(df /cassandra_data | awk '\''/cassandra_data/ {print $5}'\'' | tr -d "%")"')
echo "$(echo 100 '-' $SPACE | bc -l)% free"
WARNFREE=$(echo 100 '-' $SPACE '>' 25 | bc -l)
ERRORFREE=$(echo 100 '-' $SPACE '>' 15 | bc -l)

if [ "$ERRORFREE" -lt 1 ]
then
  MSG=$(echo "$(datestamp) ERROR: Low Metrics Cassandra Space - $(echo 100 '-' $SPACE | bc -l)% free")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Cassandra Space on `hostname`" $MAILFROM $MAILTO
elif [ "$WARNFREE" -lt 1 ]
then
  MSG=$(echo "$(datestamp) WARN: Low Metrics Cassandra Space - $(echo 100 '-' $SPACE | bc -l)% free")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for Cassandra Space on `hostname`" $MAILFROM $MAILTO
else
  MSG=$(echo "$(datestamp) INFO: Metrics Cassandra Space - $(echo 100 '-' $SPACE | bc -l)% free")
  echo -e "${MSG}"
fi

{% raw %}
# Check that all nodes are Ready
# Now with additional Finite State Automata support
# One failure in a row will only log WARN to log file. Two or
# more consecutive failures in a row will trigger ERROR alert.
echo "$(datestamp) Check for NodeNotReady"
get_state nodestatus NODE_READY_STATE
CURRENT_NODE_STATE=$(oc get nodes -o go-template="{{range .items}}{{.metadata.name}}{{print \"\t\"}}{{range .status.conditions}}{{if eq .type \"Ready\"}}{{.status}}{{end}}{{end}}{{print \"\n\"}}{{end}}")


if [[ "$(test_if_all_nodes_are_ready)" -eq "0" ]]; then
# build MSG buffer differently depending on each state.

  if [[ "${NODE_READY_STATE}" -eq "0" ]]; then
# node ready failure while in state 0, bumps us to state 1 and warns only
    set_state nodestatus 1
    MSG=$(echo "$(datestamp) WARN: Some nodes are not Ready")
    MSG="${MSG}\n${CURRENT_NODE_STATE}"
    MSG="${MSG}\n$(oc get events -n default)"
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for NodeNotReady on `hostname`" $MAILFROM ${MAILTO}
  elif [[ "${NODE_READY_STATE}" -eq "1" ]]; then
# node ready failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state nodestatus 2
    MSG=$(echo "$(datestamp) ERROR: Some nodes are not Ready")
    MSG="${MSG}\n${CURRENT_NODE_STATE}"
    MSG="${MSG}\n`oc get events -n default`"
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for NodeNotReady on `hostname`" $MAILFROM  ${MAILTO}
  elif [[ "${NODE_READY_STATE}" -eq "2" ]]; then
# node ready failure while in state 2, no state change. Still will alert though.
    MSG=$(echo "$(datestamp) ERROR: Some nodes are not Ready")
    MSG="${MSG}\n${CURRENT_NODE_STATE}"
    MSG="${MSG}\n`oc get events -n default`"
    echo -e "${MSG}"
  fi
else
# All nodes are ready, grab the node count and then adjust state accordingly.
  NUM_NODES_READY=$( return_node_ready_count )
  if [[ "${NODE_READY_STATE}" -eq "0" ]]; then
# node ready success while in state 0, no state change
    echo "$(datestamp) INFO: ${NUM_NODES_READY} nodes online and ready"
  elif [[ "${NODE_READY_STATE}" -eq "1" ]]; then
# node ready success while in state 1, state change back to 0
    set_state nodestatus 0
    echo "$(datestamp) INFO: ${NUM_NODES_READY} nodes online and ready"
  elif [[ "${NODE_READY_STATE}" -eq "2" ]]; then
# node ready success while in state 2, state change back to 0
    set_state nodestatus 0
    echo "$(datestamp) INFO: ${NUM_NODES_READY} nodes online and ready"
  fi
fi

# Verify the Registry is on 3 nodes
NODES=$(oc -n default get pods -l deploymentconfig=docker-registry -o yaml -o go-template="{{range .items}}{{if eq .status.phase \"Running\"}}{{.spec.nodeName}}{{print \"\n\"}}{{end}}{{end}}")
POD_COUNT=$(echo "${NODES}" | wc -l)
UNIQ_NODES=$(echo "${NODES}" | sort | uniq | wc -l)
echo "$(datestamp) Check that Registry is scaled to 3: ${POD_COUNT}"
echo "$(datestamp) Check that Registry is scaled to all infra nodes: " ${UNIQ_NODES}
get_state registrypodcount REGISTRY_POD_STATE
if [[ ("${POD_COUNT}" -lt 3) || ("${UNIQ_NODES}" -lt 3) ]]; then

  if [[ "${REGISTRY_POD_STATE}" -eq "0" ]]; then
# health check failure while in state 0, bumps us to state 1 and warns only
    set_state registrypodcount 1
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) WARN: Registry pod count ${POD_COUNT} is less than 3 and Registry on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) WARN: Registry pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) WARN: Registry running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for registry pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=docker-registry)"
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for RegistryPodCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${REGISTRY_POD_STATE}" -eq "1" ]]; then
# health check failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state registrypodcount 2
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: Registry pod count ${POD_COUNT} is less than 3 and Registry running on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: Registry pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: Registry running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for registry pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=docker-registry)"
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for RegistryPodCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${REGISTRY_POD_STATE}" -eq "2" ]]; then
# health check failure while in state 2, no state change. Still will alert though.
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: Registry pod count ${POD_COUNT} is less than 3 and Registy running on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: Registry pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: Registry running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for registry pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=docker-registry)"
    echo -e "${MSG}"
  fi
else
# Health check passed, adjust state accordingly.
  if [[ "${REGISTRY_POD_STATE}" -eq "0" ]]; then
# health check success while in state 0, no state change
    echo "$(datestamp) INFO: Registry pods are properly scaled"
  elif [[ "${REGISTRY_POD_STATE}" -eq "1" ]]; then
# metrics health check success while in state 1, state change back to 0
    set_state registrypodcount 0
    echo "$(datestamp) INFO: Registry pods are properly scaled"
  elif [[ "${REGISTRY_POD_STATE}" -eq "2" ]]; then
# metrics health check success while in state 2, state change back to 0
    set_state registrypodcount 0
    echo "$(datestamp) INFO: Registry pods are properly scaled"
  fi
fi

# Verify the Router is on 3 nodes
NODES=$(oc -n default get pods -l deploymentconfig=router -o yaml -o go-template="{{range .items}}{{if eq .status.phase \"Running\"}}{{.spec.nodeName}}{{print \"\n\"}}{{end}}{{end}}")
POD_COUNT=$(echo "${NODES}" | wc -l)
UNIQ_NODES=$(echo "${NODES}" | sort | uniq | wc -l)
echo "$(datestamp) Check that Router is scaled to 3: ${POD_COUNT}"
echo "$(datestamp) Check that Router is scaled to all infra nodes: " ${UNIQ_NODES}
get_state routerpodcount ROUTER_POD_STATE
if [[ ("${POD_COUNT}" -lt 3) || ("${UNIQ_NODES}" -lt 3) ]]; then
  if [[ "${ROUTER_POD_STATE}" -eq "0" ]]; then
# health check failure while in state 0, bumps us to state 1 and warns only
    set_state routerpodcount 1
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) WARN: Router pod count ${POD_COUNT} is less than 3 and Router on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) WARN: Router pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) WARN: Router running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for Router pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=router)"
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for RouterPodCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${ROUTER_POD_STATE}" -eq "1" ]]; then
# health check failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state routerpodcount 2
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: Router pod count ${POD_COUNT} is less than 3 and Router running on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: Router pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: Router running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for Router pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=router)"
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for RouterPodCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${ROUTER_POD_STATE}" -eq "2" ]]; then
# health check failure while in state 2, no state change. Still will alert though.
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: Router pod count ${POD_COUNT} is less than 3 and Router running on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: Router pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: Router running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for Router pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=router)"
    echo -e "${MSG}"
  fi
else
# Health check passed, adjust state accordingly.
  if [[ "${ROUTER_POD_STATE}" -eq "0" ]]; then
# health check success while in state 0, no state change
    echo "$(datestamp) INFO: Router pods are properly scaled"
  elif [[ "${ROUTER_POD_STATE}" -eq "1" ]]; then
# metrics health check success while in state 1, state change back to 0
    set_state routerpodcount 0
    echo "$(datestamp) INFO: Router pods are properly scaled"
  elif [[ "${ROUTER_POD_STATE}" -eq "2" ]]; then
# metrics health check success while in state 2, state change back to 0
    set_state routerpodcount 0
    echo "$(datestamp) INFO: Router pods are properly scaled"
  fi
fi

# Verify the IP Failover is on 3 nodes
{% endraw %}
ROUTER_DC="{{ router_dc }}"
{% raw %}
NODES=$(oc -n default get pods -l deploymentconfig="${ROUTER_DC}" -o yaml -o go-template="{{range .items}}{{if eq .status.phase \"Running\"}}{{.spec.nodeName}}{{print \"\n\"}}{{end}}{{end}}")
POD_COUNT=$(echo "${NODES}" | wc -l)
UNIQ_NODES=$(echo "${NODES}" | sort | uniq | wc -l)
echo "$(datestamp) Check that IPF is scaled to 3: ${POD_COUNT}"
echo "$(datestamp) Check that IPF is scaled to all infra nodes: " ${UNIQ_NODES}
get_state ipfpodcount IPF_POD_STATE
if [[ ("${POD_COUNT}" -lt 3) || ("${UNIQ_NODES}" -lt 3) ]]; then

  if [[ "${IPF_POD_STATE}" -eq "0" ]]; then
# health check failure while in state 0, bumps us to state 1 and warns only
    set_state ipfpodcount 1
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) WARN: IPF pod count ${POD_COUNT} is less than 3 and IPF running on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) WARN: IPF pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) WARN: IPF running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for IPF pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=${ROUTER_DC})"
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for IPFPodCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${IPF_POD_STATE}" -eq "1" ]]; then
# health check failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state ipfpodcount 2
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: IPF pod count ${POD_COUNT} is less than 3 and IPF running on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: IPF pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: IPF running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for IPF pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=${ROUTER_DC})"
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for IPFPodCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${IPF_POD_STATE}" -eq "2" ]]; then
# health check failure while in state 2, no state change. Still will alert though.
    if [[ ("${POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: IPF pod count ${POD_COUNT} is less than 3 and IPF running on too few infra nodes: " ${NODES})
    elif [[ "${POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: IPF pod count ${POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: IPF running on too few infra nodes " ${NODES})
    fi
# for convenience include actual pod status for IPF pods to save time investigating
    MSG="${MSG}\n$(oc -n default get pods -o wide -l deploymentconfig=${ROUTER_DC})"
    echo -e "${MSG}"
  fi
else
# Health check passed, adjust state accordingly.
  if [[ "${IPF_POD_STATE}" -eq "0" ]]; then
# health check success while in state 0, no state change
    echo "$(datestamp) INFO: IPF pods are properly scaled"
  elif [[ "${IPF_POD_STATE}" -eq "1" ]]; then
# metrics health check success while in state 1, state change back to 0
    set_state ipfpodcount 0
    echo "$(datestamp) INFO: IPF pods are properly scaled"
  elif [[ "${IPF_POD_STATE}" -eq "2" ]]; then
# metrics health check success while in state 2, state change back to 0
    set_state ipfpodcount 0
    echo "$(datestamp) INFO: IPF pods are properly scaled"
  fi
fi

# Check the clusters capacity
echo "$(datestamp) Check Cluster Capacity of region=app"
# Get all the nodes in region=app
# Format into a string with each node enclosed in quotes
NODE_LIST=$(oc get nodes -l region=app --no-headers -o=custom-columns=NAME:.metadata.name)
NODES=
for N in ${NODE_LIST}; do NODES="${NODES} \"${N}\""; done
# Get Node capacity
# node.dmz cpu_int mem_ki pods
NODE_CAP=$(oc get nodes -o go-template="{{range .items}}{{if  eq .metadata.name ${NODES}}}{{.metadata.name}}{{print \"\t\"}}{{.status.capacity.cpu}}{{print \"\t\"}}{{.status.capacity.memory}}{{print \"\t\"}}{{.status.capacity.pods}}{{print \"\n\"}}{{end}}{{end}}")

# CPU Capacity
CPU_CAP=$(echo "${NODE_CAP}" | awk -F "\t" 'BEGIN { SUM=0 } {if ($2 ~ "m") SUM+=$2/1000; else SUM+=$2 } END {print SUM}')

# Memory Capacity in Gi
MEM_CAP=$(echo "${NODE_CAP}" | awk -F "\t" 'BEGIN { SUM=0 } {if ($3 ~ "Gi") $3=$3*1024"Mi"; if ($3 ~ "G") $3=$3*1000"M"; if ($3 ~ "Mi") $3=$3*1024"Ki"; if ($3 ~ "M") $3=$3*1000"K"; if ($3 ~ "Ki") $3=$3*1024; if ($3 ~ "K") $3=$3*1000; SUM+=$3} END { print SUM/1024/1024/1024"Gi"}')

# Pod Capacity
POD_CAP=$(echo "${NODE_CAP}" | awk -F "\t" 'BEGIN { SUM=0 } {SUM+=$4 } END {print SUM}')

# Get stats for running pods
PODS=$(oc get pods --all-namespaces -o go-template="{{range .items}}{{if .spec.nodeName}}{{if and (not (eq .status.phase \"Failed\" \"Succeeded\")) (eq .spec.nodeName ${NODES})}}{{range .spec.containers}}{{.name}}{{print \"\t\"}}{{.resources.requests.cpu}}{{print \"\t\"}}{{.resources.requests.memory}}{{print \"\n\"}}{{end}}{{end}}{{end}}{{end}}")

# CPU Requests
CPU_REQ=$(echo "${PODS}" | awk -F "\t" 'BEGIN { SUM=0 } {if ($2 ~ "m") SUM+=$2/1000; else SUM+=$2 } END {print SUM}')

# Memory Requests in Gi
MEM_REQ=$(echo "${PODS}" | awk -F "\t" 'BEGIN { SUM=0 } {if ($3 ~ "Gi") $3=$3*1024"Mi"; if ($3 ~ "G") $3=$3*1000"M"; if ($3 ~ "Mi") $3=$3*1024"Ki"; if ($3 ~ "M") $3=$3*1000"K"; if ($3 ~ "Ki") $3=$3*1024; if ($3 ~ "K") $3=$3*1000; SUM+=$3} END { print SUM/1024/1024/1024"Gi"}')

# Pods Running
POD_REQ=$(echo "${PODS}" | wc -l)

get_state clustercpucapacity CPU_CAPACITY_STATE
get_state clusterramcapacity MEM_CAPACITY_STATE
get_state clusterpodcapacity POD_CAPACITY_STATE

# Print CPU Capacity
CPU_PERCENT=$(echo "${CPU_REQ} / ${CPU_CAP} * 100" | bc -l | awk '{printf "%.2f",$0}')
echo "$(datestamp) Cluster CPU Capacity: ${CPU_REQ} / ${CPU_CAP} = ${CPU_PERCENT}%"
CPU_PERCENT=$(echo "${CPU_PERCENT}" | awk '{printf "%.f",$0}')
if [ "${CPU_PERCENT}" -gt ${CPU_CAPACITY_THRESHOLD} ]; then
  if [[ "${CPU_CAPACITY_STATE}" -eq "0" ]]; then
# CPU capacity failure while in state 0, bumps us to state 1 and warns only
    set_state clustercpucapacity 1
    MSG=$(echo "$(datestamp) WARN: Cluster low on CPU capacity - over the threshold of ${CPU_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for CPUCapacityCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${CPU_CAPACITY_STATE}" -eq "1" ]]; then
# CPU capacity failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state clustercpucapacity 2
    MSG=$(echo "$(datestamp) ERROR: Cluster low on CPU capacity - over the threshold of ${CPU_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for CPUCapacityCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${CPU_CAPACITY_STATE}" -eq "2" ]]; then
    MSG=$(echo "$(datestamp) ERROR: Cluster low on CPU capacity - over the threshold of ${CPU_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  fi
else
# CPU Capacity check passed, adjust state accordingly.
  if [[ "${CPU_CAPACITY_STATE}" -eq "0" ]]; then
# CPU Capacity check success while in state 0, no state change
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient CPU capacity - under the threshold of ${CPU_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  elif [[ "${CPU_CAPACITY_STATE}" -eq "1" ]]; then
# CPU Capacity check success while in state 1, state change back to 0
    set_state clustercpucapacity 0
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient CPU capacity - under the threshold of ${CPU_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  elif [[ "${CPU_CAPACITY_STATE}" -eq "2" ]]; then
# CPU Capacity check success while in state 2, state change back to 0
    set_state clustercpucapacity 0
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient CPU capacity - under the threshold of ${CPU_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  fi
fi

# Print Memory Capacity
MEM_PERCENT=$(echo "${MEM_REQ} / ${MEM_CAP} * 100" | sed 's#Gi##g' | bc -l | awk '{printf "%.2f",$0}')
echo "$(datestamp) Cluster Memory Capacity: ${MEM_REQ} / ${MEM_CAP} = ${MEM_PERCENT}%"
MEM_PERCENT=$(echo "${MEM_PERCENT}" | awk '{printf "%.f",$0}')

if [ "${MEM_PERCENT}" -gt ${MEM_CAPACITY_THRESHOLD} ]; then
  if [[ "${MEM_CAPACITY_STATE}" -eq "0" ]]; then
# Memory capacity failure while in state 0, bumps us to state 1 and warns only
    set_state clusterramcapacity 1
    MSG=$(echo "$(datestamp) WARN: Cluster low on Memory capacity - over the threshold of ${MEM_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for MemoryCapacityCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${MEM_CAPACITY_STATE}" -eq "1" ]]; then
# Memory capacity failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state clusterramcapacity 2
    MSG=$(echo "$(datestamp) ERROR: Cluster low on Memory capacity - over the threshold of ${MEM_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for MemoryCapacityCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${MEM_CAPACITY_STATE}" -eq "2" ]]; then
    MSG=$(echo "$(datestamp) ERROR: Cluster low on Memory capacity - over the threshold of ${MEM_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  fi
else
# Memory Capacity check passed, adjust state accordingly.
  if [[ "${MEM_CAPACITY_STATE}" -eq "0" ]]; then
# Memory Capacity check success while in state 0, no state change
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient Memory capacity - under the threshold of ${MEM_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  elif [[ "${MEM_CAPACITY_STATE}" -eq "1" ]]; then
# Memory Capacity check success while in state 1, state change back to 0
    set_state clusterramcapacity 0
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient Memory capacity - under the threshold of ${MEM_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  elif [[ "${MEM_CAPACITY_STATE}" -eq "2" ]]; then
# Memory Capacity check success while in state 2, state change back to 0
    set_state clusterramcapacity 0
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient Memory capacity - under the threshold of ${MEM_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  fi
fi

# Print Pod Capacity
POD_PERCENT=$(echo "$POD_REQ / $POD_CAP * 100" | bc -l | awk '{printf "%.2f",$0}')
echo "$(datestamp) Cluster Pod Capacity: $POD_REQ / $POD_CAP = ${POD_PERCENT}%"
POD_PERCENT=$(echo "$POD_PERCENT" | awk '{printf "%.f",$0}')

if [ "${POD_PERCENT}" -gt ${POD_CAPACITY_THRESHOLD} ]; then
  if [[ "${POD_CAPACITY_STATE}" -eq "0" ]]; then
# Pod capacity failure while in state 0, bumps us to state 1 and warns only
    set_state clusterpodcapacity 1
    MSG=$(echo "$(datestamp) WARN: Cluster low on Pod capacity - over the threshold of ${POD_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for PodCapacityCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${POD_CAPACITY_STATE}" -eq "1" ]]; then
# Pod capacity failure while in state 1, bumps us to state 2 and alerts oncall with error
    set_state clusterpodcapacity 2
    MSG=$(echo "$(datestamp) ERROR: Cluster low on Pod capacity - over the threshold of ${POD_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for PodCapacityCheck on $(hostname)" $MAILFROM ${MAILTO}
  elif [[ "${POD_CAPACITY_STATE}" -eq "2" ]]; then
    MSG=$(echo "$(datestamp) ERROR: Cluster low on Pod capacity - over the threshold of ${POD_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  fi
else
# Pod Capacity check passed, adjust state accordingly.
  if [[ "${POD_CAPACITY_STATE}" -eq "0" ]]; then
# Pod Capacity check success while in state 0, no state change
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient Pod capacity - under the threshold of ${POD_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  elif [[ "${POD_CAPACITY_STATE}" -eq "1" ]]; then
# Pod Capacity check success while in state 1, state change back to 0
    set_state clusterpodcapacity 0
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient Pod capacity - under the threshold of ${POD_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  elif [[ "${POD_CAPACITY_STATE}" -eq "2" ]]; then
# Pod Capacity check success while in state 2, state change back to 0
    set_state clusterpodcapacity 0
    MSG=$(echo "$(datestamp) INFO: Cluster has sufficient Pod capacity - under the threshold of ${POD_CAPACITY_THRESHOLD}%")
    echo -e "${MSG}"
  fi
fi

# core CNS health checks - individual volume health check is out of scope here.

for POD in $(oc -n cns get pods -l glusterfs=pod --no-headers -o=custom-columns=NAME:.metadata.name)
do

  # Check health of additional services in Gluster pods
  for SERVICE in tcmu-runner gluster-block-target gluster-blockd
  do
    echo -n "$(datestamp) Status of ${SERVICE} on ${POD}: "
    if ! STATUS=$(oc exec -n cns ${POD} -- /bin/bash -c 'systemctl is-active '${SERVICE}'.service' 2>&1)
    then
        MSG=$(echo "$(datestamp) WARN: ${SERVICE} on CNS pod ${POD} is unhealthy")
        MSG="${MSG}\n${STATUS}\n$(oc exec -n cns ${POD} -- /bin/bash -c 'systemctl status '${SERVICE}'.service' 2>&1 || true)"
        echo -e "${MSG}"
        echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for CNS service check on pod ${POD}" $MAILFROM ${MAILTO}
    else
        echo "$(datestamp) INFO: ${SERVICE} on CNS pod ${POD} is healthy"
    fi
  done

  # Check Peer Status
  echo -n "$(datestamp) Confirm CNS pod ${POD} sees two connected peers: "
  PEERS=$(oc exec -n cns ${POD} -- gluster peer status | grep 'State: Peer in Cluster (Connected)' | wc -l)
  echo $PEERS
  if [ "${PEERS}" -lt 2 ]
  then
    MSG=$(echo "$(datestamp) WARN: CNS pod ${POD} not connected to peers")
    echo -e "${MSG}"
    MSG="${MSG}\n$(oc exec -n cns ${POD} -- gluster peer status)"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for CNS peer check on pod ${POD}" $MAILFROM ${MAILTO}
  else
    echo "$(datestamp) INFO: CNS pod ${POD} can see all peers"
  fi

  # Check Pool List
  echo -n "$(datestamp) Confirm CNS pod ${POD} sees 3 pool members: "
  POOLS=$(oc exec -n cns ${POD} -- gluster pool list | grep Connected | wc -l)
  echo ${POOLS}
  if [ "${POOLS}" -lt 3 ]
  then
    MSG=$(echo "$(datestamp) WARN: CNS pod ${POD} not connected to pool members")
    echo -e "${MSG}"
    MSG="${MSG}\n$(oc exec -n cns ${POD} -- gluster pool list)"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for CNS pool check on pod ${POD}" $MAILFROM ${MAILTO}
  else
    echo "$(datestamp) INFO: CNS pod ${POD} is connected to all pool members"
  fi

 # Check disk space for the gluster pod to see if it ran out.
  PODDF=$((oc exec -n cns ${POD} -- /bin/df /) | grep dev | awk '{ print $(NF-1) }' | sed 's/\%//')
  echo "$(datestamp) Checking available disk space on Gluster pod image ${POD}"
  if [ "${PODDF}" -gt "${GLUSTER_POD_THRESHOLD}" ]
  then
    MSG=$(echo "$(datestamp) WARN: CNS pod ${POD} is low on disk space. ${PODDF}% full")
    echo -e "${MSG}"
    MSG="${MSG}\n$(oc exec -n cns ${POD} -- /bin/df /)"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for CNS disk space check on pod ${POD}" $MAILFROM ${MAILTO}
  else
    echo "$(datestamp) INFO: CNS pod is ${PODDF}% full"
  fi
done

# Verify master-api pods are running on all master nodes
echo "$(datestamp) master-api Pod Status: "
NODES=$(oc get pods -n kube-system --no-headers -o=custom-columns=POD:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase --selector openshift.io/component=api)
RUNNING_POD_COUNT=$(echo "${NODES}" | grep -i running | wc -l)
UNIQ_NODES=$(echo "${NODES}" | awk '{ print $2 }' | sort | uniq | wc -l)
echo "${NODES}"
if [[ ("${RUNNING_POD_COUNT}" -lt 3) || ("${UNIQ_NODES}" -lt 3) ]]; then
    if [[ ("${RUNNING_POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-api pod count ${RUNNING_POD_COUNT} is less than 3 and master-api pod running on too few master nodes")
    elif [[ "${RUNNING_POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-api pod count ${RUNNING_POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-api pod running on too few master nodes")
    fi
    echo -e "${MSG}"
    MSG="${MSG}\n$(echo "${NODES}")"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for master-api Pod Check" $MAILFROM ${MAILTO}
else
  if [[ "${RUNNING_POD_COUNT}" -eq 3 ]]; then
    MSG=$(echo "$(datestamp) INFO: master-api Pod count ${RUNNING_POD_COUNT} is correct - all pods running")
    echo -e "${MSG}"
  fi
  if [[ "${UNIQ_NODES}" -eq 3 ]]; then
    MSG=$(echo "$(datestamp) INFO: master-api node count ${UNIQ_NODES} is correct - one API pod per master node is running")
    echo -e "${MSG}"
  fi
fi

# Verify master-etcd pods are running on all master nodes
echo "$(datestamp) master-etcd Pod Status: "
NODES=$(oc get pods -n kube-system --no-headers -o=custom-columns=POD:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase --selector openshift.io/component=etcd)
RUNNING_POD_COUNT=$(echo "${NODES}" | grep -i running | wc -l)
UNIQ_NODES=$(echo "${NODES}" | awk '{ print $2 }' | sort | uniq | wc -l)
echo "${NODES}"
if [[ ("${RUNNING_POD_COUNT}" -lt 3) || ("${UNIQ_NODES}" -lt 3) ]]; then
    if [[ ("${RUNNING_POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-etcd pod count ${RUNNING_POD_COUNT} is less than 3 and master-etcd pod running on too few master nodes")
    elif [[ "${RUNNING_POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-etcd pod count ${RUNNING_POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-etcd pod running on too few master nodes")
    fi
    echo -e "${MSG}"
    MSG="${MSG}\n$(echo "${NODES}")"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for master-etcd Pod Check" $MAILFROM ${MAILTO}
else
  if [[ "${RUNNING_POD_COUNT}" -eq 3 ]]; then
    MSG=$(echo "$(datestamp) INFO: master-etcd Pod count ${RUNNING_POD_COUNT} is correct - all pods running")
    echo -e "${MSG}"
  fi
  if [[ "${UNIQ_NODES}" -eq 3 ]]; then
    MSG=$(echo "$(datestamp) INFO: master-etcd node count ${UNIQ_NODES} is correct - one etcd pod per master node is running")
    echo -e "${MSG}"
  fi
fi

# Verify master-controllers pods are running on all master nodes
echo "$(datestamp) master-controllers Pod Status: "
NODES=$(oc get pods -n kube-system --no-headers -o=custom-columns=POD:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase --selector openshift.io/component=controllers)
RUNNING_POD_COUNT=$(echo "${NODES}" | grep -i running | wc -l)
UNIQ_NODES=$(echo "${NODES}" | awk '{ print $2 }' | sort | uniq | wc -l)
echo "${NODES}"
if [[ ("${RUNNING_POD_COUNT}" -lt 3) || ("${UNIQ_NODES}" -lt 3) ]]; then
    if [[ ("${RUNNING_POD_COUNT}" -lt 3) && ("${UNIQ_NODES}" -lt 3) ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-controllers pod count ${RUNNING_POD_COUNT} is less than 3 and master-controllers pod running on too few master nodes")
    elif [[ "${RUNNING_POD_COUNT}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-controllers pod count ${RUNNING_POD_COUNT} is less than 3")
    elif [[ "${UNIQ_NODES}" -lt 3 ]]; then
      MSG=$(echo "$(datestamp) ERROR: master-controllers pod running on too few master nodes")
    fi
    echo -e "${MSG}"
    MSG="${MSG}\n$(echo "${NODES}")"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for master-controllers Pod Check" $MAILFROM ${MAILTO}
else
  if [[ "${RUNNING_POD_COUNT}" -eq 3 ]]; then
    MSG=$(echo "$(datestamp) INFO: master-controllers Pod count ${RUNNING_POD_COUNT} is correct - all pods running")
    echo -e "${MSG}"
  fi
  if [[ "${UNIQ_NODES}" -eq 3 ]]; then
    MSG=$(echo "$(datestamp) INFO: master-controllers node count ${UNIQ_NODES} is correct - one master-controllers pod per master node is running")
    echo -e "${MSG}"
  fi
fi

# Verify Openvswitch ovs pods are running on all nodes
echo "$(datestamp) Openvswitch ovs Pod Status: "
NODES=$(oc get pods -n openshift-sdn --no-headers -o=custom-columns=POD:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase --selector app=ovs)
RUNNING_POD_COUNT=$(echo "${NODES}" | grep -i running | wc -l)
UNIQ_NODES=$(echo "${NODES}" | awk '{ print $2 }' | sort | uniq | wc -l)
TOTAL_NODES=$(oc get nodes --no-headers -o=custom-columns=NAME:.metadata.name | wc -l)
echo "${NODES}"

if [[ ("${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}") || ("${UNIQ_NODES}" -lt "${TOTAL_NODES}") ]]; then
    if [[ ("${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}") && ("${UNIQ_NODES}" -lt "${TOTAL_NODES}") ]]; then
      MSG=$(echo "$(datestamp) ERROR:  ovs pod count ${RUNNING_POD_COUNT} is less than ${TOTAL_NODES} and ovs pod running on too few nodes")
    elif [[ "${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}" ]]; then
      MSG=$(echo "$(datestamp) ERROR: ovs pod count ${RUNNING_POD_COUNT} is less than ${TOTAL_NODES}")
    elif [[ "${UNIQ_NODES}" -lt "${TOTAL_NODES}" ]]; then
      MSG=$(echo "$(datestamp) ERROR: ovs pod running on too few nodes")
    fi
    echo -e "${MSG}"
    MSG="${MSG}\n$(echo "${NODES}")"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Openvswitch ovs Pod Check" $MAILFROM ${MAILTO}
else
  if [[ "${RUNNING_POD_COUNT}" -eq "${UNIQ_NODES}" ]]; then
    MSG=$(echo "$(datestamp) INFO: ovs Pod count ${RUNNING_POD_COUNT} is correct - all Openvswitch pods running")
    echo -e "${MSG}"
  fi
  if [[ "${UNIQ_NODES}" -eq "${TOTAL_NODES}" ]]; then
    MSG=$(echo "$(datestamp) INFO: ovs node count ${UNIQ_NODES} is correct - one Openvswitch pod per node is running")
    echo -e "${MSG}"
  fi
fi

# Verify SDN pods are running on all nodes
echo "$(datestamp) SDN Pod Status: "
NODES=$(oc get pods -n openshift-sdn --no-headers -o=custom-columns=POD:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase --selector app=sdn)
RUNNING_POD_COUNT=$(echo "${NODES}" | grep -i running | wc -l)
UNIQ_NODES=$(echo "${NODES}" | awk '{ print $2 }' | sort | uniq | wc -l)
TOTAL_NODES=$(oc get nodes --no-headers -o=custom-columns=NAME:.metadata.name | wc -l)
echo "${NODES}"

if [[ ("${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}") || ("${UNIQ_NODES}" -lt "${TOTAL_NODES}") ]]; then
    if [[ ("${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}") && ("${UNIQ_NODES}" -lt "${TOTAL_NODES}") ]]; then
      MSG=$(echo "$(datestamp) ERROR: sdn pod count ${RUNNING_POD_COUNT} is less than ${TOTAL_NODES} and sdn pod running on too few nodes")
    elif [[ "${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}" ]]; then
      MSG=$(echo "$(datestamp) ERROR: sdn pod count ${RUNNING_POD_COUNT} is less than ${TOTAL_NODES}")
    elif [[ "${UNIQ_NODES}" -lt "${TOTAL_NODES}" ]]; then
      MSG=$(echo "$(datestamp) ERROR: sdn pod running on too few nodes")
    fi
    echo -e "${MSG}"
    MSG="${MSG}\n$(echo "${NODES}")"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for SDN Pod Check" $MAILFROM ${MAILTO}
else
  if [[ "${RUNNING_POD_COUNT}" -eq "${UNIQ_NODES}" ]]; then
    MSG=$(echo "$(datestamp) INFO: sdn Pod count ${RUNNING_POD_COUNT} is correct - all SDN pods running")
    echo -e "${MSG}"
  fi
  if [[ "${UNIQ_NODES}" -eq "${TOTAL_NODES}" ]]; then
    MSG=$(echo "$(datestamp) INFO: sdn node count ${UNIQ_NODES} is correct - one SDN pod per node is running")
    echo -e "${MSG}"
  fi
fi

# Verify node sync pods are running on all nodes
echo "$(datestamp) Node sync Pod Status: "
NODES=$(oc get pods -n openshift-node --no-headers -o=custom-columns=POD:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase --selector app=sync)
RUNNING_POD_COUNT=$(echo "${NODES}" | grep -i running | wc -l)
UNIQ_NODES=$(echo "${NODES}" | awk '{ print $2 }' | sort | uniq | wc -l)
TOTAL_NODES=$(oc get nodes --no-headers -o=custom-columns=NAME:.metadata.name | wc -l)
echo "${NODES}"

if [[ ("${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}") || ("${UNIQ_NODES}" -lt "${TOTAL_NODES}") ]]; then
    if [[ ("${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}") && ("${UNIQ_NODES}" -lt "${TOTAL_NODES}") ]]; then
      MSG=$(echo "$(datestamp) ERROR: sync pod count ${RUNNING_POD_COUNT} is less than ${TOTAL_NODES} and sync pod running on too few nodes")
    elif [[ "${RUNNING_POD_COUNT}" -lt "${TOTAL_NODES}" ]]; then
      MSG=$(echo "$(datestamp) ERROR: sync pod count ${RUNNING_POD_COUNT} is less than ${TOTAL_NODES}")
    elif [[ "${UNIQ_NODES}" -lt "${TOTAL_NODES}" ]]; then
      MSG=$(echo "$(datestamp) ERROR: sync pod running on too few nodes")
    fi
    echo -e "${MSG}"
    MSG="${MSG}\n$(echo "${NODES}")"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Node Sync Pod Check" $MAILFROM ${MAILTO}
else
  if [[ "${RUNNING_POD_COUNT}" -eq "${UNIQ_NODES}" ]]; then
    MSG=$(echo "$(datestamp) INFO: sync Pod count ${RUNNING_POD_COUNT} is correct - all Node sync pods running")
    echo -e "${MSG}"
  fi
  if [[ "${UNIQ_NODES}" -eq "${TOTAL_NODES}" ]]; then
    MSG=$(echo "$(datestamp) INFO: sync node count ${UNIQ_NODES} is correct - one Node sync pod per node is running")
    echo -e "${MSG}"
  fi
fi

{% endraw %}
{% endif %}


# 2018June25 disabled this entire check by request from Steven until it can
# be investigated and fixed.
# Steven is now tracking this as an issue.
# https://github.com/BCDevOps/openshift-tools/issues/13

#{% if ansible_fqdn in groups['storage'] %}
#{% raw %}
# Check Gluster disk usage
#export LVM_SUPPRESS_FD_WARNINGS=1
#echo "$(datestamp) Checking Gluster Disks"
# Find the volume groups
#VGS=$(/sbin/vgs --readonly --noheadings -o vg_name | egrep 'vg_[0-9a-f]{32}')
#for VG in $VGS
#do
#  VG_STATS=$(/sbin/vgs --units g --nosuffix --separator " " --readonly --options vg_size,vg_free --noheadings $VG )
#  SIZE=$(echo $VG_STATS | awk '{print $1}')
#  FREE=$(echo $VG_STATS | awk '{print $2}')
#  USED=$(echo "$SIZE - $FREE" | bc -l)
#  PERCENT=$(echo "$USED / $SIZE * 100" | bc -l | awk '{printf "%.2f",$0}')
#  echo "$(datestamp) $VG = ${USED}G / ${SIZE}G = $PERCENT%"
#  PERCENT=$(echo "$PERCENT" | awk '{printf "%.f",$0}')
#  if [ "$PERCENT" -gt 85 ]
#  then
#    echo "$(datestamp) ERROR: Gluster device $VG low on space"
#  fi
#done
#{% endraw %}
#{% endif %}

##### CHECKS FOR ALL OPENSHIFT NODES #####

# Check Docker is running
echo -n "$(datestamp) Docker Deamon Status: "
if ! STATUS=$(systemctl is-active docker.service 2>&1)
then
  echo -e "${STATUS}"
  MSG=$(echo "$(datestamp) ERROR: Docker is not running")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Docker Deamon Status Check on `hostname`" $MAILFROM $MAILTO
  # Other checks depend on Docker running, exit out
  exit 1
else
  echo -e "${STATUS}"
  MSG=$(echo "$(datestamp) INFO: Docker service is running")
  echo -e "${MSG}"
fi

# Check Docker Data usage
echo -n "$(datestamp) Docker Data Space is: "
SPACE=$(echo $(echo \"$(docker info 2>/dev/null | awk '/Data Space Available/ {print $4}') / $(docker info 2>/dev/null | awk '/Data Space Total/ {print $4}')\" | bc -l))
echo "$SPACE = $(echo $SPACE '*' 100 | bc -l | awk '{printf "%.2f",$0}')% free"
FREE=$(echo $SPACE '>' 0.15 | bc -l)
if [ "$FREE" -lt 1 ]
then
  MSG=$(echo "$(datestamp) ERROR: Low Docker Data Space")
  MSG="{MSG}\n$(docker info | grep 'Data Space')"
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Docker Data Space Check on `hostname`" $MAILFROM $MAILTO
fi

# Check Docker Metadata Usage
echo -n "$(datestamp) Docker Metadata Space is: "
SPACE=$(echo $(echo \"$(docker info 2>/dev/null | awk '/Metadata Space Available/ {print $4}') / $(docker info 2>/dev/null | awk '/Metadata Space Total/ {print $4}')\" | bc -l))
echo "$SPACE = $(echo $SPACE '*' 100 | bc -l | awk '{printf "%.2f",$0}')% free"
FREE=$(echo $SPACE '>' 0.15 | bc -l)
if [ "$FREE" -lt 1 ]
then
  MSG=$(echo "$(datestamp) ERROR: Low Docker Metadata Space")
  MSG="{MSG}\n$(docker info | grep 'Metadata Space')"
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Docker Metadata Space Check on `hostname`" $MAILFROM $MAILTO
fi

# Check for local volume count
# May fail on masters that have zero, so ||true
VOLS=$(mount|grep -c openshift.local.volumes||true)
echo "$(datestamp) Pod volumes: $VOLS"
if [ "$VOLS" -gt "1000" ]
then
  MSG=$( echo "$(datestamp) ERROR: Too many volumes")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for PodVolume Check on `hostname`" $MAILFROM $MAILTO
fi

# Check Node service
echo -n "$(datestamp) OpenShift Node Deamon Status: "
if ! STATUS=$(systemctl is-active atomic-openshift-node.service 2>&1)
then
  echo -e "${STATUS}"
  MSG=$(echo "$(datestamp) ERROR: OpenShift Node is not running")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Node Deamon Status Check on `hostname`" $MAILFROM $MAILTO
else
  echo -e "${MSG}"
fi


# Check for core dump files
echo "$(datestamp) Check for presence of Core files"
if STATUS=$(cd / && ls -lht $(/sbin/sysctl kernel.core_pattern | awk '{print $3}')*);
then
  MSG=$(echo -e "$(datestamp) ERROR: Core files found\n${STATUS}")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Core File Check on `hostname`" $MAILFROM $MAILTO
fi

# Check and return applicable PIDs matching docker-containerd-current. If nothing is returned, then it may have been killed off.

DOCKERPID=$(pgrep --full /usr/bin/docker-containerd-current || true)
get_state dockerpid DOCKER_PROCESS_STATE
echo "$(datestamp) Checking for active PID on docker-containerd-current"
if [ -z ${DOCKERPID} ]; then
  if [[ "${DOCKER_PROCESS_STATE}" -eq "0" ]]; then
# docker PID check failure while in state 0, bumps us to state 1 and warns as well as emails
    set_state dockerpid 1
    MSG=$(echo "$(datestamp) WARN: Cannot find any PID for docker-containerd-current")
    echo -e "${MSG}"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} WARN event for DockerPIDCheck on `hostname`" $MAILFROM $MAILTO
  elif [[ "${DOCKER_PROCESS_STATE}" -eq "1" ]]; then
# docker PID check failure while in state 1, bumps us to state 2 which remains as WARN, no email
    set_state dockerpid 2
    MSG=$(echo "$(datestamp) WARN: Cannot find any PID for docker-containerd-current")
    echo -e "${MSG}"
  elif [[ "${DOCKER_PROCESS_STATE}" -eq "2" ]]; then
# docker PID check failure while in state 2, no state change. Still produces WARN, no email.
    MSG=$(echo "$(datestamp) WARN: Cannot find any PID for docker-containerd-current")
    echo -e "${MSG}"
  fi
else
# docker PID check passed, adjust state accordingly.
  if [[ "${DOCKER_PROCESS_STATE}" -eq "0" ]]; then
# docker PID check success while in state 0, no state change
    MSG=$(echo "$(datestamp) INFO: PID found for docker-containerd-current")
    echo -e "${MSG}"
  elif [[ "${DOCKER_PROCESS_STATE}" -eq "1" ]]; then
# docker PID check success while in state 1, state change back to 0
    set_state dockerpid 0
    MSG=$(echo "$(datestamp) INFO: PID found for docker-containerd-current")
    echo -e "${MSG}"
    MSG="${MSG}\n$(echo "Found PID ${DOCKERPID}")"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} RECOVER event for DockerPIDCheck on `hostname`" $MAILFROM $MAILTO
  elif [[ "${DOCKER_PROCESS_STATE}" -eq "2" ]]; then
# docker PID check success while in state 2, state change back to 0
    set_state dockerpid 0
    MSG=$(echo "$(datestamp) INFO: PID found for docker-containerd-current")
    echo -e "${MSG}"
    MSG="${MSG}\n$(echo "Found PID ${DOCKERPID}")"
    echo -e "${MSG}" | mail -s "${EMAIL_ENV} RECOVER event for DockerPIDCheck on `hostname`" $MAILFROM $MAILTO
  fi
fi

# Check disk space for /var/lib/origin, alert if over 80% full

echo -e "$(datestamp) Testing /var/lib/origin disk usage: "
SPACE=$(df /var/lib/origin | awk '/origin/ {print $5}' | tr -d "%")
FREE=$(echo 100 '-' $SPACE '>' 20 | bc -l)
if [ "$FREE" -lt 1 ]
then
  MSG=$(echo "$(datestamp) ERROR: Low Disk Space in /var/lib/origin - $(echo 100 '-' $SPACE | bc -l)% free")
  echo -e "${MSG}"
  echo -e "${MSG}" | mail -s "${EMAIL_ENV} ERROR event for Registry Space on `hostname`" $MAILFROM $MAILTO
else
  MSG=$(echo "$(datestamp) INFO: Sufficient Disk Space in /var/lib/origin - $(echo 100 '-' $SPACE | bc -l)% free")
  echo -e "${MSG}"
fi

